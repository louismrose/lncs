\section{Evaluation}
\label{sec:evaluation}
For the evaluation of the MBSecMon specification and generation process for AUTOSAR, an adaptation of the Simulink example model \emph{Automatic Transmission Controller}~\cite{TheMathWorks2012}, as presented in Sect.~\ref{sec:example}, is used.
In order to provide compatibility with Simulink's AUTOSAR code generator, which does not support the continuous blocks (such as integrators) that were used in the original example, we decomposed the example model into separate software components and replaced the incompatible blocks with their corresponding discrete versions.
The generated code contains the runnables (executable entities) of each software component (SW-C), which have to be integrated into the implementation code skeleton that was generated by the system level design and simulation tool OptXware Embedded Architect (OXEA).
In OXEA, we have also designed the system model that corresponds to the Simulink example, and which is stored in the standardized ARXML format.

We instrumented the evaluation system with two different monitors (cf. Fig.~\ref{fig:simulinkExample}).
The first one is \emph{AllowedSpeedChanges}, as presented in Sect.~\ref{sec:mbsecmonFramework}, which monitors the communication between the components \emph{Vehicle} and \emph{ShiftLogic} in order to detect a communication error between these components.
The monitor signals an error in case that it detects a difference between two consecutive speed readings that is larger than 10\,mph within a 20\,ms timeframe (the period of the tasks).
The second one is \emph{InvalidGearChanges}, which, in contrast to the first signature, monitors the misuse case of a gear shifting by more than one step within a 20\,ms timeframe. Therefore, the communication between the components \emph{ShiftLogic} and \emph{Transmission} is monitored.
Based on these two signatures, a monitor that consists of a controller and a monitor representation for each signature is generated by the MBSecMon process.

The evaluation covers the run-time overhead that the monitoring induces per instrumented port (\emph{Gear} for ShiftLogic and Transmission, and \emph{VehicleSpeed} for Vehicle and ShiftLogic), and per instrumented runnable (Vehicle, ShiftLogic, Transmission).
Furthermore, we analyze the memory overhead of the monitors, for both, code and data segments.
Finally, a scalability analysis on an embedded system is performed.

% Additionally to the signature \emph{AllowedSpeedChanges} presented in Sect.~\ref{sec:mbsecmonFramework}, a second signature \emph{InvalidGearChanges} is added. 
% In contrast to the first, this signature describes a misuse case, where a gear shift of more than one step at once is detected.
% Based on these two signatures, a monitor that consists of a controller and a monitor representation for each signature is generated by the MBSecMon process. 

\subsubsection{Run-time analysis.} The evaluation was conducted using OptXware EA's simulation environment on a AMD Phenom II X4 955 processor, running at 3.20\,GHz.
The timing measurements were taken using the Win32 API functions \emph{MyQueryPerformanceCounter} and \emph{MyQueryPerformanceFrequency}, which are Window's high resolution timing functions, providing CPU tick granularity.
This is a best effort solution, as there is no commonly agreed on reference architecture for such evaluation.
Therefore, we also provide relative measurements of the execution time overhead as comparison.

Figure~\ref{fig:simulationInput} shows the input data (readings of the throttle and brake torque sensor) for the test run of the automatic transmission model.
It represents a passing maneuver, where the vehicle approaches a slower car and then abruptly accelerates to pass the car.

\begin{figure}[tb]
\centering
\subfloat{
\pgfplotstableread{sections/PassingManeuver.dat}{\pistonkinetics}
\begin{tikzpicture}[scale=0.6]
\begin{axis}[xlabel={Time in s},  scaled x ticks = false, ylabel={Throttle in $\%$}, height=4cm, width=9cm, ymin=-5, ymax=110]
\addplot [black,very thick] table [x=time, y=throttle, col sep=semicolon] {sections/PassingManeuver.dat};
\end{axis}
\end{tikzpicture}
}
\qquad
\subfloat{
\begin{tikzpicture}[scale=0.6]
\begin{axis}[xlabel={Time in s}, ylabel={Brake torque in $\%$}, height=4cm, width=9cm, , ymin=-5, ymax=110]
\addplot [black,very thick] table [x=time, y=break, col sep=semicolon] {sections/PassingManeuver.dat};
\end{axis}
\end{tikzpicture}
}

\caption{Input data set for the passing maneuver}
\label{fig:simulationInput}
\end{figure}


% \begin{figure}[htp]
% \begin{center}
%   \includegraphics[width=.6\textwidth]{SimulationInput.jpg}
%   \caption{Passing Maneuver}
%   \label{fig:simulationInput}
% \end{center}
% \end{figure}

Table~\ref{tab:resultsO0RteCall} shows the run-time for performing calls to the RTE in the simulation environment without (\emph{Original}) and with instrumentation (\emph{Instr.}).
The measured time for the instrumented RTE calls includes the wrapper around the call method, the invocation of the monitor, and the evaluation of the event by the monitor.
The overhead is between 28 and 31\% for write calls and between 49 and 57\% for read calls.
This difference results from the structure of the signatures that include an additional assignment or condition in the monitor for the transmitted value at the receiving side (read call).



\begin{table}[tb]
\centering\setlength{\tabcolsep}{3pt}
\scriptsize
 \caption{Execution time comparison of original and monitored calls to the RTE}
 \begin{tabular}{llccc}\toprule
 &  & \multicolumn{2}{c}{\textbf{Average}}&\\
 \textbf{Component}&\textbf{RTE Call}  & \emph{Original} \emph{(ticks)} & \emph{Instr. (ticks)} & \emph{Diff. ($\%$)}\\
% $\begin{array}{l} \mbox{\emph{Original}}\\\mbox{\emph{(ticks)}}\end{array}$ & $\begin{array}{l} \mbox{\emph{Instr.}}\\\mbox{\emph{(ticks)}}\end{array}$ & $\begin{array}{l} \mbox{\emph{Diff.}}\\\mbox{\emph{($\%$)}}\end{array}$\\
 \cmidrule(r){1-2}\cmidrule(r){3-4}\cmidrule(r){5-5}
ShiftLogic() & Rte\_Read\_VehicleSpeed \ldots 	& 24,76 & 38,87 &57\\
ShiftLogic() & Rte\_Write\_Gear \ldots 		& 18,00 & 23,60 &31\\
Transmission() & Rte\_Read\_Gear \ldots 		& 25,28 & 37,63 &49\\
Vehicle() & Rte\_Write\_VehicleSpeed \ldots 	& 21,56 & 27,62 &28\\\bottomrule
 \end{tabular}
 \label{tab:resultsO0RteCall}
\end{table}


Table~\ref{tab:resultsO0Runnable} shows the influence of the monitor on the total run-time of the \mbox{SW-C's} runnables.
For the ShiftLogic component, two ports have been wrapped and, therefore, the instrumented runnable calls the monitor twice as often as for the other components.
The components Transmission and Vehicle, which contain only one instrumented port, therefore, have a smaller run-time overhead.
%Despite the two monitor signatures having comparable complexity, the run-time overhead differs.
%This results from the optimizations done by the C code compiler, that only works for the Transmission component.
%\TODO{\ldots optimierter Code entspricht nicht Erwartung und tabelle \ref{tab:resultsO0RteCall}} 
%Halte ich für einen Leser der die Interna nicht kennt für schwer nachvollziehbar. Würde es daher, um Verwirrung zu vermeiden, eher rauslassen.
\begin{table}[tb]
\centering\setlength{\tabcolsep}{4pt}
\scriptsize
 \caption{Execution time comparison of original and monitored runnables}
 \begin{tabular}{lcccc}\toprule
 &   \multicolumn{2}{c}{\textbf{Average}}&&\\
\textbf{Component} & \emph{Original (ticks)} & \emph{Instr. (ticks)} & \emph{Diff. (ticks)}& \emph{Diff. ($\%$)}\\   
%$\begin{array}{l} \mbox{\emph{Original}}\\\mbox{\emph{(ticks)}}\end{array}$ & $\begin{array}{l} \mbox{\emph{Instr.}}\\\mbox{\emph{(ticks)}}\end{array}$ & $\begin{array}{l} \mbox{\emph{Diff.}}\\\mbox{\emph{(ticks)}}\end{array}$&$\begin{array}{l} \mbox{\emph{Diff.}}\\\mbox{\emph{($\%$)}}\end{array}$\\
\cmidrule(r){1-1}\cmidrule(r){2-3}\cmidrule(r){4-5}
ShiftLogic()  	& 56,03 & 70,97	&14,94&28\\
Transmission()  & 57,28 & 63,77	&6,49&11\\
Vehicle() 		& 48,63 & 58,39 &9,76&20\\\bottomrule
 \end{tabular}
 \label{tab:resultsO0Runnable}
\end{table}

\subsubsection{Memory overhead.} We conducted our analysis of the memory overhead using the tool \emph{objdump} of the GNU binutils toolsuite on the compiled object files.
\mbox{Objdump} provides a detailed overview of the memory consumption of the text (aka. code) section, and the three data sections data, bss (uninitialized data) and rdata (read-only data).
The analysis of the memory overhead of the integrated monitors is depicted in Tab.~\ref{tab:memoryOverhead}.
The first four columns show the memory that is consumed by the RTE call wrappers that pass the signals to the monitor.
As the functionality of each wrapper is similar, their overhead is constant.

The monitor component consists of a controller (\emph{Contr.}) that manages the monitors and the signatures AllowedSpeedChanges (\emph{ASC}) and InvalidGearChan\-ges (\emph{IGC}).
The controller caches the transmitted values, triggers the signature representations, and evaluates their results.
%Das ist so nicht korrekt. In AUTOSAR gibt es keinen dynamischen Speicher. Sollten dennoch irgendwelche Aufrufe zu zB malloc() vorhanden sein wäre das ganze auf einem echten AUTOSAR System nicht lauffähig!
%Wir sollten die Darstellungsweise der Data-Section nochmal überdenken; die Bedeutung der Zahlen in Klammern ist nicht erklärt.
%Will das ungern ohne Rücksprache mit euch entscheiden, daher lass ich es erstmal so.
The memory overhead of the data section is very small and grows very slowly with the complexity of the signatures because only the state of the monitor and the monitor specific variables, as shown in Sect.~\ref{sec:example}, are stored there.
%As stated at the evaluation of the run-time, the signatures differ in their execution time caused by the optimisation of the C compiler.
%This is also reflected in the code size.
%The signature \emph{ASC}, whose execution time is smaller, has a higher static memory foot print.  
%obige 3 zeilen sind nicht zutreffend

\begin{table}[tb]
\centering\setlength{\tabcolsep}{4pt}
\scriptsize
\caption{Memory overhead caused by the monitors in byte}
 \begin{tabular}{lccccccc}\toprule
 &   \multicolumn{4}{c}{\textbf{Wrapper}}&\multicolumn{3}{c}{\textbf{Monitor}}\\
\textbf{Type} &   \emph{Read\_VS} &\emph{Write\_VS} &\emph{Read\_G} & \emph{Write\_G}&\emph{Contr.}&\emph{ASC}&\emph{IGC}\\\cmidrule(r){1-1}\cmidrule(r){2-5}\cmidrule(r){6-8}
Code  & 48	& 48	& 48	& 48 	& 1632 	& 2512	& 2160\\
Data  & 0 	& 0 	& 0 	& 0		& 88 	& 40 	& 32 \\\bottomrule
% Data  & 0 	& 0 	& 0 	& 0		& 88 (92) 	& 30 (336)	& 30 (240)\\\bottomrule
 \end{tabular}
 
 \label{tab:memoryOverhead}
\end{table}


\subsubsection{Scalability analysis.} The previous results have shown that the generated monitors can be used in an AUTOSAR environment with reasonable overhead. 
For the evaluation of the scalability of the monitors, various models of different complexity are generated and the run-time behaviour of the generated C code is measured on a Fujitsu SK-16FX-100PMC evaluation board equipped with an F$^2$MC-16FX MB96340 series microcontroller (16\,bit, 56\,MHz).
Table~\ref{tab:evaluation} shows the different models, the run-time of the generated monitors for processing one event in the signature, where a message  consists of a sending and receiving event, and the needed memory on the micro controller.
The values for the needed data memory (RAM) include approximately 800 bytes of data that do not origin from the generated monitor.
For the measurement of the run-time, 1000 complete runs of the signatures have been performed.
Complex conditions and actions in the signature are dismissed and only the monitor itself is measured. 

\begin{table}[tb]
\begin{center}
\setlength{\tabcolsep}{3.5pt}
\scriptsize
\caption{Results of the scalability evaluation}
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textit{Model} &\textit{\#Messages} &\textit{Different} \textit{events}&  \textit{Run-time/}\textit{Event in} $\mu s$ & \textit{Code} \textit{in byte}& \textit{Data} \textit{in byte} \\
\cmidrule(){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(){6-6}
 M1 &  1  & 2            & 28,384 &896 & 1038\\
 M2 &  2  & 4            & 25,360 &1078 &1044\\
 M3 &  3  & 6            & 24,277  &1230 &1044\\
 M4 &  4 &  8            & 23,728  &1382 &1044\\
 M5 &  50  & 100       & 22,636 & 8898&1154\\     
 M6 &  100  & 200       & 22,660  & 17026&1268\\
 M7 & 	2  & 2           & 25,936  &1043 &1044\\
 M8 & 	3  & 2           & 25,099  &1158 &1044\\
 M9& 	4  & 2           & 25,176  &1562 &1044\\
 M10&  50  & 2         &  46,239 & 7260&1156\\
 M11&  100  & 2         &  67,973 & 13605&1270\\
 M12&  2 * M4 & 8   & 46,424  &2696 &1056\\
 M13&  3 * M4 & 8   & 67,992   &4000 &1068\\
 M14& 4 * M4 & 8   & 89,744  & 5304&1080\\
\bottomrule
\end{tabular}
\label{tab:evaluation}
\end{center}
\end{table}

Model 1 to 6 demonstrate how the monitors scale if the number of messages increases in the signature. 
Model 1 and 7 to 11 show the overhead when all messages have the same message type.
In Model 4 and 12 to 14, the number of signatures of constant size (M4) is increased.
With an increasing number of processing steps to reach the final state of the signature the initialization overhead gets less important. 
The code memory consumption increases linearly with the number of messages located in a signature.
In all cases, the RAM needed to store the state of the signature increases very slowly, because the state of the signature is binary coded.
This is an important factor for use on resource constrained embedded systems.
The evaluation shows that the monitors have a constant computing time per processed event (M1 to M6) and only a linear increase for processing an event that is annotated at multiple transitions (M1, M7 to M11). 


% used instrumentation method and why
% fault injection
% \begin{itemize}
%   \item Overhead: Memory and run-time, maybe SLOC
%   \item SWIFI experiment for validation of the monitor
%   \item Overhead of the monitor/instrumentation\cite{Piper2012}
% \end{itemize}
% 
% 
% Conclusions of the evaluation:
% \begin{itemize}
%   \item low overhead (monitor and instrumentation), is usable on embedded systems
%   \item Errors due to integration of system information into modeling process not likely
%   \item What is with the modeling?
%   \item The overhead of the monitoring code has to be considered in the scheduling.
%   \item Possibility to monitor black box components, because only communication between components is regarded 
%   \item \ldots
% \end{itemize}